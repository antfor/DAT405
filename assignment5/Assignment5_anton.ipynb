{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def value(s,sp, Ra, V, gamma, P ): \n",
    "    \n",
    "    if(sp[0] < 0 or sp[0] >= len(Ra[0])): # FLYTTA ?\n",
    "        return -1\n",
    "       \n",
    "    if(sp[1] < 0 or sp[1] >= len(Ra)):\n",
    "        return -1\n",
    "    \n",
    "    return P*(Ra[sp[0]][sp[1]]+ gamma * V[sp[0]][sp[1]]) + (1-P)* (Ra[s[0]][s[1]] + gamma * V[s[0]][s[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Note. You need to install gym! Sometimes difficult on Windows. Google for advise.\n",
    "import gym\n",
    "import numpy as np\n",
    "import random\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BellmanStep(Ra, V, gamma, P):\n",
    "    Vn = [x[:] for x in V] # todo fix \n",
    "    \n",
    "    for row in range(len(V)):\n",
    "        \n",
    "        for col in range(len(V[0])):\n",
    "            maxL = []  \n",
    "            s = (row,col)\n",
    "            E = (s[0]+1,s[1])\n",
    "            W = (s[0]-1,s[1])\n",
    "            N = (s[0],s[1]+1)\n",
    "            S = (s[0],s[1]-1)\n",
    "            action = [E,W,N,S]\n",
    "            \n",
    "            for sp in action:\n",
    "                maxL.append(value(s,sp, Ra, V, gamma, P))\n",
    "            maxValue = max(maxL)\n",
    "            Vn[row][col] = maxValue\n",
    "    \n",
    "    return Vn                     \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Bellman(Ra, V, gamma, P, i):\n",
    "    \n",
    "    for x in range(i):\n",
    "        V = BellmanStep(Ra, V, gamma, P);\n",
    "        \n",
    "\n",
    "    return V\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Bellman(Ra, V, gamma, P):\n",
    "    while(True):\n",
    "        Vn = BellmanStep(Ra, V, gamma, P);\n",
    "        if (Vn == V):\n",
    "            break;\n",
    "        V = Vn\n",
    "        \n",
    "    return V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[45.61292366170413, 51.948051948051926, 45.61292366170413],\n",
       " [51.948051948051926, 48.05194805194803, 51.948051948051926],\n",
       " [45.61292366170413, 51.948051948051926, 45.61292366170413]]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Ra = [[0,0,0],[0,10,0],[0,0,0]]\n",
    "V = [[0,0,0],[0,0,0],[0,0,0]] \n",
    "gamma = 0.9\n",
    "P = 0.8\n",
    "i = 10000\n",
    "\n",
    "Bellman(Ra, V, gamma, P)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bellQ(s, sp, Ra, V, gamma):\n",
    "    if(sp == -1):\n",
    "        sp = s\n",
    "    if(sp == len(V)):\n",
    "        sp = s\n",
    "    \n",
    "    return (Ra[s][sp]+ gamma * V[sp])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BellmanQstep(Ra, V, gamma):\n",
    "    Vn = V[:] # todo fix \n",
    "    actionTable = []\n",
    "    for s in range(len(V)):\n",
    "        maxL = []\n",
    "        step = s +1\n",
    "        back = 0\n",
    "        action = [step,back]\n",
    "        for sp in action:\n",
    "            maxL.append(bellQ(s, sp, Ra, V, gamma))\n",
    "        maxValue = max(maxL)\n",
    "        Vn[s] = maxValue\n",
    "        actionTable.append(maxL)\n",
    "                   \n",
    "    return (Vn,actionTable)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BellmanQ(Ra, V, gamma):\n",
    "    Vn = ()\n",
    "    while(True):\n",
    "        Vn = BellmanQstep(Ra, V, gamma);\n",
    "        if (Vn[0] == V):\n",
    "            break;\n",
    "        V = Vn[0]\n",
    "        \n",
    "    return Vn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[162.9012499999996, 156.7561874999996],\n",
       " [171.4749999999996, 156.7561874999996],\n",
       " [180.49999999999957, 156.7561874999996],\n",
       " [189.99999999999955, 156.7561874999996],\n",
       " [199.99999999999955, 156.7561874999996]]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Ra = [[2,0,0,0,0],[2,0,0,0,0],[2,0,0,0,0],[2,0,0,0,0],[2,0,0,0,10]]\n",
    "V = [0,0,0,0,0]\n",
    "gamma = 0.95\n",
    "BellmanQ(Ra, V, gamma)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = gym.make('NChain-v0',slip=0.0)\n",
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_episodes = 1000 #20000 #60000\n",
    "gamma = 0.95 #0.99\n",
    "learning_rate = 0.1 #0.95 #0.85\n",
    "epsilon = 0.5#1 #0.15 #0.1\n",
    "\n",
    "# initialize the Q table\n",
    "Q = np.zeros([5, 2])\n",
    "Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in range(num_episodes):\n",
    "\tstate = env.reset()\n",
    "\tdone = False\n",
    "\twhile done == False:\n",
    "        # First we select an action:\n",
    "\t\tif random.uniform(0, 1) < epsilon: # Flip a skewed coin\n",
    "\t\t\taction = env.action_space.sample() # Explore action space\n",
    "\t\telse:\n",
    "\t\t\taction = np.argmax(Q[state,:]) # Exploit learned values\n",
    "        # Then we perform the action and receive the feedback from the environment\n",
    "\t\tnew_state, reward, done, info = env.step(action)\n",
    "        # Finally we learn from the experience by updating the Q-value of the selected action\n",
    "\t\tupdate = reward + (gamma*np.max(Q[new_state,:])) - Q[state, action]\n",
    "\t\tQ[state,action] += learning_rate*update \n",
    "\t\tstate = new_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[162.90125  , 156.7561875],\n",
       "       [171.475    , 156.7561875],\n",
       "       [180.5      , 156.7561875],\n",
       "       [190.       , 156.7561875],\n",
       "       [200.       , 156.7561875]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
