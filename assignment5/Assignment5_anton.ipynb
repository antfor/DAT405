{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def value(s,sp, Ra, V, gamma, P ): \n",
    "    \n",
    "    if(sp[0] < 0 or sp[0] >= len(Ra[0])): # FLYTTA ?\n",
    "        return -1\n",
    "       \n",
    "    if(sp[1] < 0 or sp[1] >= len(Ra)):\n",
    "        return -1\n",
    "    \n",
    "    return P*(Ra[sp[0]][sp[1]]+ gamma * V[sp[0]][sp[1]]) + (1-P)* (Ra[s[0]][s[1]] + gamma * V[s[0]][s[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Note. You need to install gym! Sometimes difficult on Windows. Google for advise.\n",
    "import gym\n",
    "import numpy as np\n",
    "import random\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BellmanStep(Ra, V, gamma, P):\n",
    "    Vn = [x[:] for x in V] # todo fix \n",
    "    \n",
    "    for row in range(len(V)):\n",
    "        \n",
    "        for col in range(len(V[0])):\n",
    "            maxL = []  \n",
    "            s = (row,col)\n",
    "            E = (s[0]+1,s[1])\n",
    "            W = (s[0]-1,s[1])\n",
    "            N = (s[0],s[1]+1)\n",
    "            S = (s[0],s[1]-1)\n",
    "            action = [E,W,N,S]\n",
    "            \n",
    "            for sp in action:\n",
    "                maxL.append(value(s,sp, Ra, V, gamma, P))\n",
    "            maxValue = max(maxL)\n",
    "            Vn[row][col] = maxValue\n",
    "    \n",
    "    return Vn                     \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Bellman(Ra, V, gamma, P, i):\n",
    "    \n",
    "    for x in range(i):\n",
    "        V = BellmanStep(Ra, V, gamma, P);\n",
    "        \n",
    "\n",
    "    return V\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Bellman(Ra, V, gamma, P):\n",
    "    while(True):\n",
    "        Vn = BellmanStep(Ra, V, gamma, P);\n",
    "        if (Vn == V):\n",
    "            break;\n",
    "        V = Vn\n",
    "        \n",
    "    return V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[45.61292366170413, 51.948051948051926, 45.61292366170413],\n",
       " [51.948051948051926, 48.05194805194803, 51.948051948051926],\n",
       " [45.61292366170413, 51.948051948051926, 45.61292366170413]]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Ra = [[0,0,0],[0,10,0],[0,0,0]]\n",
    "V = [[0,0,0],[0,0,0],[0,0,0]] \n",
    "gamma = 0.9\n",
    "P = 0.8\n",
    "i = 10000\n",
    "\n",
    "Bellman(Ra, V, gamma, P)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bellQ(s, sp, Ra, V, gamma):\n",
    "    if(sp == -1):\n",
    "        sp = s\n",
    "    if(sp == len(V)):\n",
    "        sp = s\n",
    "    \n",
    "    return (Ra[s][sp]+ gamma * V[sp])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BellmanQstep(Ra, V, gamma):\n",
    "    Vn = V[:] # todo fix \n",
    "    actionTable = []\n",
    "    for s in range(len(V)):\n",
    "        maxL = []\n",
    "        step = s +1\n",
    "        back = 0\n",
    "        action = [step,back]\n",
    "        for sp in action:\n",
    "            maxL.append(bellQ(s, sp, Ra, V, gamma))\n",
    "        maxValue = max(maxL)\n",
    "        Vn[s] = maxValue\n",
    "        actionTable.append(maxL)\n",
    "                   \n",
    "    return (Vn,actionTable)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BellmanQ(Ra, V, gamma):\n",
    "    Vn = ()\n",
    "    while(True):\n",
    "        Vn = BellmanQstep(Ra, V, gamma);\n",
    "        if (Vn[0] == V):\n",
    "            break;\n",
    "        V = Vn[0]\n",
    "        \n",
    "    return Vn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[162.9012499999996, 156.7561874999996],\n",
       " [171.4749999999996, 156.7561874999996],\n",
       " [180.49999999999957, 156.7561874999996],\n",
       " [189.99999999999955, 156.7561874999996],\n",
       " [199.99999999999955, 156.7561874999996]]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Ra = [[2,0,0,0,0],[2,0,0,0,0],[2,0,0,0,0],[2,0,0,0,0],[2,0,0,0,10]]\n",
    "V = [0,0,0,0,0]\n",
    "gamma = 0.95\n",
    "BellmanQ(Ra, V, gamma)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = gym.make('NChain-v0',slip=0.0)\n",
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_episodes = 1000 #20000 #60000\n",
    "gamma = 0.95 #0.99\n",
    "learning_rate = 0.1 #0.95 #0.85\n",
    "epsilon = 0.5#1 #0.15 #0.1\n",
    "\n",
    "# initialize the Q table\n",
    "Q = np.zeros([5, 2])\n",
    "Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in range(num_episodes):\n",
    "\tstate = env.reset()\n",
    "\tdone = False\n",
    "\twhile done == False:\n",
    "        # First we select an action:\n",
    "\t\tif random.uniform(0, 1) < epsilon: # Flip a skewed coin\n",
    "\t\t\taction = env.action_space.sample() # Explore action space\n",
    "\t\telse:\n",
    "\t\t\taction = np.argmax(Q[state,:]) # Exploit learned values\n",
    "        # Then we perform the action and receive the feedback from the environment\n",
    "\t\tnew_state, reward, done, info = env.step(action)\n",
    "        # Finally we learn from the experience by updating the Q-value of the selected action\n",
    "\t\tupdate = reward + (gamma*np.max(Q[new_state,:])) - Q[state, action]\n",
    "\t\tQ[state,action] += learning_rate*update \n",
    "\t\tstate = new_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[162.90125  , 156.7561875],\n",
       "       [171.475    , 156.7561875],\n",
       "       [180.5      , 156.7561875],\n",
       "       [190.       , 156.7561875],\n",
       "       [200.       , 156.7561875]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decision trees works by classifying a set by repeatedly dividing it using specific binary (boolean) classifiers or decisions. For example if you want to classify animals you can divide them using the classifier \"is bi-ped\" and then \"is herbivore\" and so forth until you reach the labeled leaf node, representing the outcome (in this example it would be the specific animal).\n",
    "\n",
    "You can also use numerical decisions such as \"weight > 100 kg\".\n",
    "\n",
    "The convention is that yes decisions are to the left and no as to the right.\n",
    "\n",
    "The set you want to classify can either be targeted to one single boolean target such as \"has or has not heart disease or a multivariate target such as different animal species.\n",
    "\n",
    "You can create a decision tree in multiple ways\n",
    "\n",
    "One way, if you want to determine the class of every \"person\" in the data set with a 100% accuracy you can make a tree where you ask every question such that you account for all combinations of decisions. This is done by asking question Q1 at the top level, then question Q2 for each answer to Q1 (so you will ask Q2 2 times) and then you ask Q3 4 time and so forth. The downside of this is that the tree is really big ($2^n$) and then can become expensive to use. Big trees like this are also bad for generalizing and predicting target classes for new data points since a big tree like this is comparable to overfitting a k-means classification. \n",
    "\n",
    "The tree will also become hard to maintain and unintuitive because we well get a lot of leaves at the bottom, each corresponding to only a few data points, even though we might want to classify several leaves as the same class. For example if our targets are \"has heart disease\" and \"has not heart disease\", then we don't need thousands of leaves, we just need 2 really (Note that you will most likely get more than 2 leaves even for better solutions but you will generally get way more with the approach described above).\n",
    "\n",
    "Another way to create a decision tree is based on the gini impurity. A question in a decision tree split the data into two child sets, one set which all had the answer yes to the question and the other set containing data with No as the answer. If one of the sets were pure this would mean it only contains data with the same results.  No further question would be needed in this case and this corresponds to a gini score of 0. A decision tree that test for a binary result, having a set with a gini score of one would mean the probability of a yes or no is 50% whitin the set.\n",
    "\n",
    "The Gini impurity of a split (question) would be the weighted average of the Gini impurity for the child sets. The root question of the tree would thus be the question with the lowest Gini impurity. The next question will be the one that lowers the gini value the most for the remaining set. This step keeps repeating until there is no more question or until the remaining question don’t lead to a lower gini impurity.\n",
    "\n",
    "The leafs can also be pure or impure and a general weakness of decision trees is there inaccuracy, i.e. they are not good at classifying new samples. Random forest is a technique that build upon decision trees that improves the accuracy.\n",
    "But there simplicity have still made them popular for medical diagnosis and in marketing and sales. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
