{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Isak Schwartz: 6 hours\n",
    "Anton Forsberg: 6 hours\n",
    "# Assignment 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import scikit-learn dataset library\n",
    "from sklearn import datasets\n",
    "\n",
    "# Import train_test_split function\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#Import Gaussian Naive Bayes model\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "#Import scikit-learn metrics module for accuracy calculation\n",
    "from sklearn import metrics\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import os\n",
    "\n",
    "import re\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "from sklearn.naive_bayes import BernoulliNB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1: Preprocessing\n",
    "We start by reading all the spam and ham mails from the given paths in the function createTokens(...). We then divide the mails into training and test data using the function train_test_split inside our custom function called printBAyesianAccuracies(). The split function distributes the mails into training and test data randomly with a distribution of 70% and 30% respectively. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 : python program\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.b : difference between multinomial and bernoulli naive bayes\n",
    "The fundemental difference between the two is that binarized bernoulli used true or false for each word (or feature) in the mails to predict if it is spam or ham, whereas multinomial uses the actual amount of times each word occurs (say 0,1 or 34 etc.. ). \n",
    "\n",
    "For bernoulli this means that a word that occurs thousands of times can give the same weight when determining if a mail is spam or ham as a word that just occurs 1 time. For multinomial, the number of occurences for a word is more important."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createTokens(pathsAndTargets,filter_Words, filter_Headers): # crates a list of tokens for all files in a given directory   \n",
    "    files = []\n",
    "    target = []\n",
    "    for pathAndTarget in pathsAndTargets: # loop through all directories (ex: hard_ham and spam_2)\n",
    "        \n",
    "        for file in os.listdir(pathAndTarget[0]): # loop through all the files in the directories \n",
    "            filePath = (pathAndTarget[0] + \"/\" ) + file # file path to the specific file\n",
    "            try:\n",
    "                email = open(filePath,'r', encoding=\"utf-8\").read()  #Read the file\n",
    "                \n",
    "                if(filter_Headers):  #remove haeders and footers (question 5) \n",
    "                    email = filetrFile(email)\n",
    "                files.append(email) #add file to the dataset\n",
    "                target.append(pathAndTarget[1]) # pathAndTarget[1] say if it is ham(1) or spam(0)\n",
    "            except UnicodeDecodeError:\n",
    "                pass\n",
    "            \n",
    "            \n",
    "    # make vector from documents       \n",
    "    vectorizer = CountVectorizer(analyzer='word')\n",
    "    dataset = vectorizer.fit_transform(files).toarray()\n",
    "    \n",
    "    if(filter_Words):#filte away common and uncommon words (question 4)\n",
    "        filterList = filterWords(vectorizer.get_feature_names(), dataset) #create list of common and uncommon words\n",
    "        vectorizer = CountVectorizer(analyzer='word', stop_words = frozenset(filterList)) # make vector from documents  \n",
    "        vectorizer._validate_vocabulary() \n",
    "        dataset = vectorizer.fit_transform(files).toarray() #create the data set\n",
    "    elif(filter_Headers):\n",
    "        vectorizer = CountVectorizer(analyzer='word', preprocessor = cleanhtml) # we give the preprocessor a function that removes html tags\n",
    "        dataset = vectorizer.fit_transform(files).toarray() #create the data set\n",
    "    \n",
    "    return (dataset, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function that removes headers and footers\n",
    "def filetrFile(file):\n",
    "    try: \n",
    "        index = file.index(\"<!-- ### footer ### -->\") # remove the footer\n",
    "        file = file[:index]\n",
    "    except ValueError:\n",
    "        pass\n",
    "    try: \n",
    "        index = file.index(\"<!-- ### \\header ### -->\")+ len(\"<!-- ### \\header ### -->\") # remove header\n",
    "        file = file[index:]\n",
    "    except ValueError:\n",
    "        pass\n",
    "\n",
    "    try:\n",
    "        index = file.index(\"\\n\\n\") # remove header for non html emails (all (moast) files ended the email header with a blank row)\n",
    "        file = file[index:]\n",
    "    except ValueError:\n",
    "        pass\n",
    "\n",
    "    try:\n",
    "        index = file.index(\"<html>\")+ len(\"<html>\") # remove header (this tag is the first thing occuring after the email header)\n",
    "        file = file[index:]\n",
    "    except ValueError:\n",
    "        pass\n",
    "    \n",
    "    return file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessor function to remove html tags\n",
    "def cleanhtml(raw_html):\n",
    "    cleanr = re.compile('<.*?>') \n",
    "    cleantext = re.sub(cleanr, ' ', raw_html) #replace everything  surrounded by tags with a white space\n",
    "    return cleantext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to filter out common and uncommn words\n",
    "def filterWords(words, occurences):\n",
    "    # give length equal to the data entries\n",
    "    totalOccurences = np.zeros(len(occurences[0]))\n",
    "    for mail in occurences:# loop throug all the email and count the total occurences of all the words\n",
    "        for i in range (len(mail)):\n",
    "            totalOccurences[i] += mail[i]\n",
    "    # make filterList\n",
    "    d = {'word': words, 'occurences': totalOccurences}\n",
    "    df = pd.DataFrame(data=d)\n",
    "    unfrequent = df.loc[df['occurences'] < 5]['word'].values # filter away words occuring less than 5 times\n",
    "    frequent = df.loc[df['occurences'] > 1000]['word'].values # filter away words occuring more than 1000 times\n",
    " \n",
    "    return np.concatenate((unfrequent , frequent)) #combine the two lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function where the data is split in to training and testing data, training the models and model evaluation is done\n",
    "def printBayesianAccuracies(pathsAndTargets, filterWords, filterHeaders):\n",
    "    dataset = createTokens(pathsAndTargets, filterWords, filterHeaders) # create the data sets\n",
    "    ham = []\n",
    "    hamTarget = []\n",
    "    spam = []\n",
    "    spamTarget = []\n",
    "    \n",
    "    for index in range(0, len(dataset[0])): #loop that sort the big dataset into the ham or spam dataset\n",
    "        if(0 == (dataset[1][index])): # spam have the value 0\n",
    "            spam.append(dataset[0][index]) #add to spam dataset\n",
    "            spamTarget.append(0)\n",
    "        else:\n",
    "            ham.append(dataset[0][index]) #add to ham dataset\n",
    "            hamTarget.append(1)\n",
    "            \n",
    "            \n",
    "    # Split datasets into training set and test set (70-30)\n",
    "    ham_train, ham_test, hamTarget_train, hamTarget_test = train_test_split(ham, hamTarget, test_size=0.3,random_state=0)\n",
    "    spam_train, spam_test, spamTarget_train, spamTarget_test = train_test_split(spam, spamTarget, test_size=0.3,random_state=0)\n",
    "\n",
    "    #create dataset that cointains all the data\n",
    "    X_train = ham_train + spam_train\n",
    "    y_train = hamTarget_train + spamTarget_train\n",
    "    \n",
    "    X_test = ham_test + spam_test\n",
    "    y_test = hamTarget_test + spamTarget_test\n",
    "    \n",
    "    #Create a Na√Øve Bayes Classifiers\n",
    "    classifiers = {(MultinomialNB(),\"Multinomial naive bayes\"), (BernoulliNB(binarize = 0), \"Bernoulli naive bayes\")}\n",
    "    \n",
    "    for classifier in classifiers:#loop through the two classifiers (Multinomial naive bayes and Bernoulli naive bayes)\n",
    "        dataset = classifier[0]\n",
    "        \n",
    "        #Train the model using the training sets\n",
    "        dataset.fit(X_train, y_train)\n",
    "\n",
    "        #Predict the response for test dataset\n",
    "        y_pred = dataset.predict(X_test)\n",
    "        \n",
    "        #Predict the response for ham dataset\n",
    "        y_pred_ham = dataset.predict(ham_test)\n",
    "        \n",
    "        #Predict the response for spam dataset\n",
    "        y_pred_spam = dataset.predict(spam_test)\n",
    "\n",
    "        # Model Accuracy, how often is the classifier correct?\n",
    "        print(\"Accuracy of \" + classifier[1] + \": \",metrics.accuracy_score(y_test, y_pred))\n",
    "        \n",
    "        # Model Accuracy, how often is the classifier correct?\n",
    "        print(\"Accuracy of\" + classifier[1] + \" on ham : \",metrics.accuracy_score(hamTarget_test, y_pred_ham))\n",
    "        \n",
    "        # Model Accuracy, how often is the classifier correct?\n",
    "        print(\"Accuracy of\" + classifier[1] + \" on spam: \",metrics.accuracy_score(spamTarget_test, y_pred_spam))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to show a list of the occurences of every word\n",
    "def wordTable(words, occurences):\n",
    "    # give length equal to the data entries\n",
    "    totalOccurences = np.zeros(len(occurences[0]))\n",
    "    for mail in occurences:\n",
    "        for i in range (len(mail)):\n",
    "            totalOccurences[i] += mail[i]\n",
    "    # make table\n",
    "    d = {'word': words, 'occurences': totalOccurences}\n",
    "    df = pd.DataFrame(data=d)\n",
    "    df.sort_values(by=['occurences'], inplace = True)\n",
    "    print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 : run program on easy and hard ham and include results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spam_2 and easy_ham_2 Accuracies:\n",
      "Accuracy of Multinomial naive bayes:  0.9505851755526658\n",
      "Accuracy ofMultinomial naive bayes on ham :  0.9948051948051948\n",
      "Accuracy ofMultinomial naive bayes on spam:  0.90625\n",
      "Accuracy of Bernoulli naive bayes:  0.9700910273081924\n",
      "Accuracy ofBernoulli naive bayes on ham :  0.9948051948051948\n",
      "Accuracy ofBernoulli naive bayes on spam:  0.9453125\n",
      "spam_2 and hard_ham Accuracies:\n",
      "Accuracy of Bernoulli naive bayes:  0.926829268292683\n",
      "Accuracy ofBernoulli naive bayes on ham :  0.582089552238806\n",
      "Accuracy ofBernoulli naive bayes on spam:  0.9869791666666666\n",
      "Accuracy of Multinomial naive bayes:  0.9556541019955654\n",
      "Accuracy ofMultinomial naive bayes on ham :  0.7611940298507462\n",
      "Accuracy ofMultinomial naive bayes on spam:  0.9895833333333334\n"
     ]
    }
   ],
   "source": [
    "easyPaths = {(\"spam_2\",0),(\"easy_ham_2\",1)} # ({(directory, spam(0)),(directory })\n",
    "hardPaths = {(\"spam_2\",0),(\"hard_ham\",1)}\n",
    "    \n",
    "print(\"spam_2 and easy_ham_2 Accuracies:\")\n",
    "printBayesianAccuracies(easyPaths, False, False)\n",
    "print(\"spam_2 and hard_ham Accuracies:\")\n",
    "printBayesianAccuracies(hardPaths, False, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 : filter out uncommon/common words\n",
    "We print a list of words and the corresponding word count for each model we crate to see which words are common and uncommon. This is done in the function wordTables(...). But to actually choose which words to exclude from the model we use one numerical threshold which is how many times a word need to occur to be included, and another threshold which is the maximum amount of times a word can occur to be included. We chose these thresholds by tweaking them little by little and seeing if the final accuracies for the model improved. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.a : why is this useful? what words are common / uncommon?\n",
    "It is useful to remove common words because they probably dont differ significantly between spam and ham. Meaning they are not a good indicator whther a mail is spam and ham. This also means these words can give other words less impact or weight when classifying mails, making classification less accurate overall. \n",
    "\n",
    "It is good to remove uncommon words because they can give strong associations to a certain type of mail even though they aren't typical for either spam or ham. For example, if a spam mail has the word \"Saturn\", which is a uncommon word that is not inherent to spam in any way, our model will think that all mails with the word saturn are spam (or at least make them way more likely to classidfied as spam). This is a problem because Saturn might be just as common in ham as spam.\n",
    "\n",
    "By looking at the gnerated word count files we see that the 5 most common words for the model done on easy ham are : com, font, to, 3d and the. There are many other common words like is, of, by and with. The results are similar for hard ham.\n",
    "\n",
    "The most uncommon words are g4q9dme14040, ntli, ntl4s72sppxz, developerworks, ntion and other seemingly random words that don't seem to be associated with either spam or ham."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.b : filter using countVectorizer, how do results differ from part 3?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spam_2 and easy_ham_2 Accuracies:\n",
      "Accuracy of Bernoulli naive bayes:  0.9856957087126138\n",
      "Accuracy ofBernoulli naive bayes on ham :  0.9948051948051948\n",
      "Accuracy ofBernoulli naive bayes on spam:  0.9765625\n",
      "Accuracy of Multinomial naive bayes:  0.988296488946684\n",
      "Accuracy ofMultinomial naive bayes on ham :  1.0\n",
      "Accuracy ofMultinomial naive bayes on spam:  0.9765625\n",
      "spam_2 and hard_ham Accuracies:\n",
      "Accuracy of Multinomial naive bayes:  0.9866962305986696\n",
      "Accuracy ofMultinomial naive bayes on ham :  0.9850746268656716\n",
      "Accuracy ofMultinomial naive bayes on spam:  0.9869791666666666\n",
      "Accuracy of Bernoulli naive bayes:  0.9290465631929047\n",
      "Accuracy ofBernoulli naive bayes on ham :  0.746268656716418\n",
      "Accuracy ofBernoulli naive bayes on spam:  0.9609375\n"
     ]
    }
   ],
   "source": [
    "easyPaths = {(\"spam_2\",0),(\"easy_ham_2\",1)} \n",
    "hardPaths = {(\"spam_2\",0),(\"hard_ham\",1)}\n",
    "    \n",
    "print(\"spam_2 and easy_ham_2 Accuracies:\")\n",
    "printBayesianAccuracies(easyPaths, True, False)\n",
    "print(\"spam_2 and hard_ham Accuracies:\")\n",
    "printBayesianAccuracies(hardPaths, True, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 : remove headers and footers, run program again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spam_2 and easy_ham_2 Accuracies:\n",
      "Accuracy of Multinomial naive bayes:  0.9687906371911573\n",
      "Accuracy ofMultinomial naive bayes on ham :  0.9974025974025974\n",
      "Accuracy ofMultinomial naive bayes on spam:  0.9401041666666666\n",
      "Accuracy of Bernoulli naive bayes:  0.9427828348504551\n",
      "Accuracy ofBernoulli naive bayes on ham :  0.9948051948051948\n",
      "Accuracy ofBernoulli naive bayes on spam:  0.890625\n",
      "spam_2 and hard_ham Accuracies:\n",
      "Accuracy of Multinomial naive bayes:  0.9534368070953437\n",
      "Accuracy ofMultinomial naive bayes on ham :  0.9552238805970149\n",
      "Accuracy ofMultinomial naive bayes on spam:  0.953125\n",
      "Accuracy of Bernoulli naive bayes:  0.9002217294900222\n",
      "Accuracy ofBernoulli naive bayes on ham :  0.5522388059701493\n",
      "Accuracy ofBernoulli naive bayes on spam:  0.9609375\n"
     ]
    }
   ],
   "source": [
    "easyPaths = {(\"spam_2\",0),(\"easy_ham_2\",1)}\n",
    "hardPaths = {(\"spam_2\",0),(\"hard_ham\",1)}\n",
    "    \n",
    "print(\"spam_2 and easy_ham_2 Accuracies:\")\n",
    "printBayesianAccuracies(easyPaths, False, True)\n",
    "print(\"spam_2 and hard_ham Accuracies:\")\n",
    "printBayesianAccuracies(hardPaths, False, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.a : do results improve from part 3 and 4?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.b : how can data set split skew results? Remedies?\n",
    "Our program always splits the spam and the ham the same way into training and test data, meaning 70% of the ham is used as training data and 70% of the spam is used as traing data. The alternative would be to merge all spam and ham into one data set and then split that into 70% training data and 30% test data. This solution won't impact the final distribution of spam and ham inside each set dramatically, it will still be around 70/70 and 30/30. \n",
    "\n",
    "Something to consider next is how many spam mails and ham mails there are overall. In our case we have about 1500 spam and 250 ham. This means that overall, mails are more likely to be classified as spam than ham COMPARED TO if there where say 1500 spam and 1500 ham GENERALLY. However, it is hard to say if this is good or bad, It might just be that 1500/250 is representative for the actual spam/ham ratio in the real world, meaning there shouldnt be the same amount of each in the training and test data either. So without further speculation, we can say that both of the proposed solutions in the paragraph above are fine and wouldn't skew the results more or less regardless of how many spam and ham mails there are in total.\n",
    "\n",
    "The other way the split could be skewed is in terms of the actual distribution of training vs test data. If you divide it so you get too little training data (e.g 10% training data and 90 test data) the bayesian model can be too specialized to the few mails that appear in the training data. This means that the model would be bad at seeing general patterns in mails and at predicting whether a given mail is spam or ham. This would become apprant when looking at the accuracy score so you can try to tweak the distribution afterwards. If the training data is too big, you dont get enough test mails to verify it with, meaning it is hard to tell how good the model is since the test data will most likely on represent a small fraction of all mails. This maens the accuracy score might still be good but will still not tell you if it predicts any mail very well. This is a bigger problem than in the reversed case since you are less likely to notice something is wrong if you have a good accuracy.\n",
    "\n",
    "To become more certain in the split distribution is good you can use cross validation. This means making similar tests such that every part of the data is used as training data in some validation, and testing data in some other validation. For example, splitting the data in 4 chunks means you can do 4 test, first use the first 25% of the data as test data and the rest as training data, for the next test use the first 26%-50% of the data as test data and the rest as training data and so forth... Lastly you take the average of each all the accuracy scores which will make it easier to tell if the model was good overall even if one single test might have given you a really high or low accuracy score. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.c : What would happen if training set was mostly spam, and test mostly ham\n",
    "You would be way more likely to classify any given mail as spam. This is because most mails are spam in the training data so the model will most likely think most mails in the test data are spam as well. This can happen even if there are a few ham mails in the training data because the model will not become good enough to see small differences betweem spam and ham mails and general patterns for ham mails, meaning many ham mails will still get classified as spam. This is obviously bad since it would make such a spam filter put important mail in the clutter folder. \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
