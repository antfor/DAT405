{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Isak Schwartz: 6 hours\n",
    "Anton Forsberg: 6 hours\n",
    "# Assignment 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1: Preprocessing\n",
    "We start by reading all the spam and ham mails from the given paths in the function createTokens(...). We then divide the mails into training and test data using the function train_test_split inside our custom function called printBAyesianAccuracies(). The split function distributes the mails into training and test data randomly with a distribution of 70% and 30% respectively. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 : python program\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.b : difference between multinomial and bernoulli naive bayes\n",
    "The fundemental difference between the two is that binarized bernoulli used true or false for each word (or feature) in the mails to predict if it is spam or ham, whereas multinomial uses the actual amount of times each word occurs (say 0,1 or 34 etc.. ). \n",
    "\n",
    "For bernoulli this means that a word that occurs thousands of times can give the same weight when determining if a mail is spam or ham as a word that just occurs 1 time. For multinomial, the number of occurences for a word is more important."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 : run program on easy and hard ham and include results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 : filter out uncommon/common words\n",
    "We print a list of words and the corresponding word count for each model we crate to see which words are common and uncommon. This is done in the function wordTables(...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.a : why is this useful? what words are common / uncommon?\n",
    "It is useful to remove common words because they probably dont differ significantly between spam and ham. Meaning they are not a good indicator whther a mail is spam and ham. This also means these words can give other words less impact or weight when classifying mails, making classification less accurate overall. \n",
    "\n",
    "It is good to remove uncommon words because they can give strong associations to a certain type of mail even though they aren't typical for either spam or ham. For example, if a spam mail has the word \"Saturn\", which is a uncommon word that is not inherent to spam in any way, our model will think that all mails with the word saturn are spam (or at least make them way more likely to classidfied as spam). This is a problem because Saturn might be just as common in ham as spam.\n",
    "\n",
    "By looking at the gnerated word count files we see that the 5 most common words for the model done on easy ham are : com, font, to, 3d and the. There are many other common words like is, of, by and with. The results are similar for hard ham.\n",
    "\n",
    "The most uncommon words are g4q9dme14040, ntli, ntl4s72sppxz, developerworks, ntion and other seemingly random words that don't seem to be associated with either spam or ham."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.b : filter using countVectorizer, how do results differ from part 3?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 : remove headers and footers, run program again"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.a : do results improve from part 3 and 4?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.b : how can data set split skew results? Remedies?\n",
    "Our program always splits the spam and the ham the same way into training and test data, meaning 70% of the ham is used as training data and 70% of the spam is used as traing data. The alternative would be to merge all spam and ham into one data set and then split that into 70% training data and 30% test data. This solution won't impact the final distribution of spam and ham inside each set dramatically, it will still be around 70/70 and 30/30. \n",
    "\n",
    "Something to consider next is how many spam mails and ham mails there are overall. In our case we have about 1500 spam and 250 ham. This means that overall, mails are more likely to be classified as spam than ham COMPARED TO if there where say 1500 spam and 1500 ham GENERALLY. However, it is hard to say if this is good or bad, It might just be that 1500/250 is representative for the actual spam/ham ratio in the real world, meaning there shouldnt be the same amount of each in the training and test data either. So without further speculation, we can say that both of the proposed solutions in the paragraph above are fine and wouldn't skew the results more or less regardless of how many spam and ham mails there are in total.\n",
    "\n",
    "The other way the split could be skewed is in terms of the actual distribution of training vs test data. If you divide it so you get too little training data (e.g 10% training data and 90 test data) the bayesian model can be too specialized to the few mails that appear in the training data. This means that the model would be bad at seeing general patterns in mails and at predicting whether a given mail is spam or ham. This would become apprant when looking at the accuracy score so you can try to tweak the distribution afterwards. If the training data is too big, you dont get enough test mails to verify it with, meaning it is hard to tell how good the model is since the test data will most likely on represent a small fraction of all mails. This maens the accuracy score might still be good but will still not tell you if it predicts any mail very well. This is a bigger problem than in the reversed case since you are less likely to notice something is wrong if you have a good accuracy.\n",
    "\n",
    "To become more certain in the split distribution is good you can use cross validation. This means making similar tests such that every part of the data is used as training data in some validation, and testing data in some other validation. For example, splitting the data in 4 chunks means you can do 4 test, first use the first 25% of the data as test data and the rest as training data, for the next test use the first 26%-50% of the data as test data and the rest as training data and so forth... Lastly you take the average of each all the accuracy scores which will make it easier to tell if the model was good overall even if one single test might have given you a really high or low accuracy score. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.c : What would happen if training set was mostly spam, and test mostly ham\n",
    "You would be way more likely to classify any given mail as spam. This is because most mails are spam in the training data so the model will most likely think most mails in the test data are spam as well. This can happen even if there are a few ham mails in the training data because the model will not become good enough to see small differences betweem spam and ham mails and general patterns for ham mails, meaning many ham mails will still get classified as spam. This is obviously bad since it would make such a spam filter put important mail in the clutter folder. \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
