{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import scikit-learn dataset library\n",
    "from sklearn import datasets\n",
    "\n",
    "# Import train_test_split function\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#Import Gaussian Naive Bayes model\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "#Import scikit-learn metrics module for accuracy calculation\n",
    "from sklearn import metrics\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import os\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "from sklearn.naive_bayes import BernoulliNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createTokens(pathsAndTargets): # crates a list of tokens for all files in a given directory   \n",
    "    files = []\n",
    "    target = []\n",
    "    for pathAndTarget in pathsAndTargets:\n",
    "        \n",
    "        for file in os.listdir(pathAndTarget[0]):\n",
    "            filePath = (pathAndTarget[0] + \"/\" ) + file\n",
    "            files.append(open(filePath,'r', encoding=\"latin-1\").read())\n",
    "            target.append(pathAndTarget[1])\n",
    "    vectorizer = CountVectorizer()\n",
    "    # make vector from documents\n",
    "    dataset = vectorizer.fit_transform(files).toarray()\n",
    "    \n",
    "    #print word count table\n",
    "    wordTable(vectorizer.get_feature_names(), dataset, pathsAndTargets)\n",
    "    \n",
    "    return (dataset, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def printBayesianAccuracies(pathsAndTargets):\n",
    "    dataset = createTokens(pathsAndTargets)\n",
    "    # Split dataset into training set and test set (70-30)\n",
    "    datasetspam datasetham targetspam targetham\n",
    "    spam_train spam_test , spamtarget_train , spamtarget_test = train_test_split(datasetspam, datasettarget)\n",
    "    ............ ham_\n",
    "    \n",
    "    x_train = spam_train.append(ham_train)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(dataset[0], dataset[1], test_size=0.3,random_state=0)\n",
    "    \n",
    "    #Create a Na√Øve Bayes Classifiers\n",
    "    classifiers = {(MultinomialNB(),\"Multinomial naive bayes\"), (x), \"Bernoulli naive bayes\")}\n",
    "    \n",
    "    for classifier in classifiers:\n",
    "        dataset = classifier[0]\n",
    "        \n",
    "        #Train the model using the training sets\n",
    "        dataset.fit(X_train, y_train)\n",
    "\n",
    "        #Predict the response for test dataset\n",
    "        y_pred = dataset.predict(X_test)\n",
    "\n",
    "        # Model Accuracy, how often is the classifier correct?\n",
    "        print(\"Accuracy of \" + classifier[1] + \": \",metrics.accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wordTable(words, occurences, name):\n",
    "    # give length equal to the data entries\n",
    "    totalOccurences = np.zeros(len(occurences[0]))\n",
    "    for mail in occurences:\n",
    "        for i in range (len(mail)):\n",
    "            totalOccurences[i] += mail[i]\n",
    "    # make table\n",
    "    d = {'word': words, 'occurences': totalOccurences}\n",
    "    df = pd.DataFrame(data=d)\n",
    "    df.reset_index()\n",
    "    df.sort_values(by=['occurences'], inplace = True)\n",
    "    #np.savetxt(name+\".txt\", df.values, fmt='%d')\n",
    "    #print df.to_csv(sep=' ', index=False, header=False)\n",
    "    #np.savetxt(r'c:\\data\\np.txt', df.values, fmt='%d')\n",
    "    #base_filename = 'Values.txt'\n",
    "    #with open(os.path.join(\"/\", base_filename),'w') as outfile:\n",
    "    #    df.to_string(outfile)\n",
    "    #with open(\"c\", 'a') as f:\n",
    "    #   f.write(df.to_string(header = False, index = False))\n",
    "    # r'c:\\data\\pandas.txt'\n",
    "    path = str(name) + \".txt\"\n",
    "    df.to_csv(path, header=None, index=None, sep='\\t', mode='a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spam_2 and easy_ham_2 Accuracies:\n",
      "Accuracy of Bernoulli naive bayes:  0.9785714285714285\n",
      "Accuracy of Multinomial naive bayes:  0.9535714285714286\n",
      "spam_2 and hard_ham Accuracies:\n",
      "Accuracy of Bernoulli naive bayes:  0.9313131313131313\n",
      "Accuracy of Multinomial naive bayes:  0.9595959595959596\n"
     ]
    }
   ],
   "source": [
    "easyPaths = {(\"spam_2\",0),(\"easy_ham_2\",1)}\n",
    "hardPaths = {(\"spam_2\",0),(\"hard_ham\",1)}\n",
    "    \n",
    "print(\"spam_2 and easy_ham_2 Accuracies:\")\n",
    "printBayesianAccuracies(easyPaths)\n",
    "print(\"spam_2 and hard_ham Accuracies:\")\n",
    "printBayesianAccuracies(hardPaths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 common words\n",
    "We use some common and uncommon words from the tables above to improve teh accuracies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.b\n",
    "You can use cross validation, meaning making similar tests such that every part of the data is used as test data in some test, and training data in some test. For example, splitting teh data in 4 chunks means you can do 4 test, first use the first 25% of the data as test data and the rest as training data, for the next test use the first 26%-50% of the data as test data and the rest as training data and so forth... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   words  col2\n",
      "0      1     3\n",
      "1      2     4\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>words</th>\n",
       "      <th>col2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   words  col2\n",
       "1      2     4\n",
       "0      1     3"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = {'words': [1, 2], 'col2': [3, 4]}\n",
    "df = pd.DataFrame(data=d)\n",
    "print(df)\n",
    "df.sort_values(by=['col2'], inplace = True, ascending=False)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Bernoulli naive bayes:  0.0\n",
      "Accuracy of Multinomial naive bayes:  0.0\n"
     ]
    }
   ],
   "source": [
    "testpath = {(\"spam_test\",0),(\"ham_test\",1)}\n",
    "printBayesianAccuracies(testpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"{('easy_ham_2', 1), ('spam_2', 0)}\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "easyPaths = {(\"spam_2\",0),(\"easy_ham_2\",1)}\n",
    "str(easyPaths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
