{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Isak Schwartz: 6 hours\n",
    "Anton Forsberg: 6 hours\n",
    "# Assignment 4\n",
    "## notes\n",
    "naive bayes\n",
    "train given spam, ham\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import scikit-learn dataset library\n",
    "from sklearn import datasets\n",
    "\n",
    "# Import train_test_split function\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#Import Gaussian Naive Bayes model\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "#Import scikit-learn metrics module for accuracy calculation\n",
    "from sklearn import metrics\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import os\n",
    "\n",
    "import re\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "from sklearn.naive_bayes import BernoulliNB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1: Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createTokens(pathsAndTargets, filterBool): # crates a list of tokens for all files in a given directory   \n",
    "    files = []\n",
    "    target = []\n",
    "    for pathAndTarget in pathsAndTargets:\n",
    "        \n",
    "        for file in os.listdir(pathAndTarget[0]):\n",
    "            filePath = (pathAndTarget[0] + \"/\" ) + file\n",
    "            try:\n",
    "                unFilteredFile = open(filePath,'r', encoding=\"utf-8\").read()\n",
    "                filteredFile = filetrFile(unFilteredFile)\n",
    "                files.append(filteredFile)\n",
    "                target.append(pathAndTarget[1])\n",
    "            except UnicodeDecodeError:\n",
    "                pass\n",
    "            \n",
    "    vectorizer = CountVectorizer(analyzer='word')\n",
    "    # make vector from documents\n",
    "    dataset = vectorizer.fit_transform(files).toarray()\n",
    "    if(filterBool):\n",
    "        filterList = filterWords(vectorizer.get_feature_names(), dataset)\n",
    "       # print(filterList)\n",
    "        vectorizer = CountVectorizer(analyzer='word', stop_words = frozenset(filterList))\n",
    "        vectorizer._validate_vocabulary()\n",
    "        dataset = vectorizer.fit_transform(files).toarray()\n",
    "        # print(vectorizer.get_feature_names())\n",
    "    \n",
    "    return (dataset, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def printBayesianAccuracies(pathsAndTargets, filterBool):\n",
    "    dataset = createTokens(pathsAndTargets, filterBool)\n",
    "    ham = []\n",
    "    hamTarget = []\n",
    "    spam = []\n",
    "    spamTarget = []\n",
    "    \n",
    "    for index in range(0, len(dataset[0])):\n",
    "        if(0 == (dataset[1][index])):\n",
    "            spam.append(dataset[0][index])\n",
    "            spamTarget.append(0)\n",
    "        else:\n",
    "            ham.append(dataset[0][index])\n",
    "            hamTarget.append(1)\n",
    "            \n",
    "            \n",
    "    # Split dataset into training set and test set (70-30)\n",
    "    #X_train, X_test, y_train, y_test = train_test_split(dataset[0], dataset[1], test_size=0.3,random_state=0)\n",
    "    ham_train, ham_test, hamTarget_train, hamTarget_test = train_test_split(ham, hamTarget, test_size=0.3,random_state=0)\n",
    "    spam_train, spam_test, spamTarget_train, spamTarget_test = train_test_split(spam, spamTarget, test_size=0.3,random_state=0)\n",
    "\n",
    "    X_train = ham_train + spam_train\n",
    "    y_train = hamTarget_train + spamTarget_train\n",
    "    \n",
    "    X_test = ham_test + spam_test\n",
    "    y_test = hamTarget_test + spamTarget_test\n",
    "    \n",
    "    #Create a Na√Øve Bayes Classifiers\n",
    "    classifiers = {(MultinomialNB(),\"Multinomial naive bayes\"), (BernoulliNB(binarize = 0), \"Bernoulli naive bayes\")}\n",
    "    \n",
    "    for classifier in classifiers:\n",
    "        dataset = classifier[0]\n",
    "        \n",
    "        #Train the model using the training sets\n",
    "        dataset.fit(X_train, y_train)\n",
    "\n",
    "        #Predict the response for test dataset\n",
    "        y_pred = dataset.predict(X_test)\n",
    "        \n",
    "        #Predict the response for ham dataset\n",
    "        y_pred_ham = dataset.predict(ham_test)\n",
    "        \n",
    "        #Predict the response for spam dataset\n",
    "        y_pred_spam = dataset.predict(spam_test)\n",
    "\n",
    "        # Model Accuracy, how often is the classifier correct?\n",
    "        print(\"Accuracy of \" + classifier[1] + \": \",metrics.accuracy_score(y_test, y_pred))\n",
    "        \n",
    "        # Model Accuracy, how often is the classifier correct?\n",
    "        #print(\"Accuracy of\" + classifier[1] + \" on ham : \",metrics.accuracy_score(hamTarget_test, y_pred_ham))\n",
    "        \n",
    "        # Model Accuracy, how often is the classifier correct?\n",
    "        #print(\"Accuracy of\" + classifier[1] + \" on spam: \",metrics.accuracy_score(spamTarget_test, y_pred_spam))\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wordTable(words, occurences):\n",
    "    # give length equal to the data entries\n",
    "    totalOccurences = np.zeros(len(occurences[0]))\n",
    "    for mail in occurences:\n",
    "        for i in range (len(mail)):\n",
    "            totalOccurences[i] += mail[i]\n",
    "    # make table\n",
    "    d = {'word': words, 'occurences': totalOccurences}\n",
    "    df = pd.DataFrame(data=d)\n",
    "    df.sort_values(by=['occurences'], inplace = True)\n",
    "    print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filterWords(words, occurences):\n",
    "    # give length equal to the data entries\n",
    "    totalOccurences = np.zeros(len(occurences[0]))\n",
    "    for mail in occurences:\n",
    "        for i in range (len(mail)):\n",
    "            totalOccurences[i] += mail[i]\n",
    "    # make filterList\n",
    "    d = {'word': words, 'occurences': totalOccurences}\n",
    "    df = pd.DataFrame(data=d)\n",
    "    unfrequent = df.loc[df['occurences'] < 4]['word'].values # filter away words occuring less than 10 times\n",
    "    frequent = df.loc[df['occurences'] > 1000]['word'].values # filter away words occuring more than 10 times\n",
    "    #print(type(unfrequent))\n",
    "    return np.concatenate((unfrequent , frequent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filetrFile(file):\n",
    "    try: \n",
    "        index = file.index(\"<!-- ### footer ### -->\") # remove the footer\n",
    "        file = file[:index]\n",
    "    except ValueError:\n",
    "        pass\n",
    "    try: \n",
    "        index = file.index(\"<!-- ### \\header ### -->\")+ len(\"<!-- ### \\header ### -->\") # remove header\n",
    "        file = file[index:]\n",
    "    except ValueError:\n",
    "        pass\n",
    "\n",
    "    try:\n",
    "        index = file.index(\"\\n\\n\") # remove header for non html emails\n",
    "        file = file[index:]\n",
    "    except ValueError:\n",
    "        pass\n",
    "\n",
    "    try:\n",
    "        index = file.index(\"<html>\")+ len(\"<html>\") # remove header\n",
    "        file = file[index:]\n",
    "    except ValueError:\n",
    "        pass\n",
    "    \n",
    "    return file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessor function to remove html tags\n",
    "def cleanhtml(raw_html):\n",
    "    cleanr = re.compile('<.*?>')\n",
    "    cleantext = re.sub(cleanr, ' ', raw_html)\n",
    "    return cleantext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.a : \"run\" entire text, including header and footer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spam_2 and easy_ham_2 Accuracies:\n",
      "Accuracy of Bernoulli naive bayes:  0.951885565669701\n",
      "Accuracy of Multinomial naive bayes:  0.9778933680104032\n",
      "spam_2 and hard_ham Accuracies:\n"
     ]
    }
   ],
   "source": [
    "easyPaths = {(\"spam_2\",0),(\"easy_ham_2\",1)}\n",
    "hardPaths = {(\"spam_2\",0),(\"hard_ham\",1)}\n",
    "    \n",
    "print(\"spam_2 and easy_ham_2 Accuracies:\")\n",
    "printBayesianAccuracies(easyPaths, True)\n",
    "print(\"spam_2 and hard_ham Accuracies:\")\n",
    "printBayesianAccuracies(hardPaths, True)\n",
    "\n",
    "#paths = {(\"spam_2\",0),(\"easy_ham_2\",1)}\n",
    "\n",
    "#dataset = createTokens(paths, True) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.b : split spam and ham in training / test sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 : python program"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.a : use 4 sets hamtrain, spamtrain, hamtest, spamtest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.b : use naive bayes, classify test sets, report % ham, spam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
